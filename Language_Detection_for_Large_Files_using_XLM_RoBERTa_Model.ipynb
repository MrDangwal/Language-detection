{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Model Source:** https://huggingface.co/papluca/xlm-roberta-base-language-detection\n",
        "\n",
        "**Purpose:** Perform Language detection for Large files that cannot be processed via googlesheets or other limited free API's. The performance and accuracy are identical to google sheets 'DETECTLANGUAGE' results."
      ],
      "metadata": {
        "id": "vmHuzAAspS-X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "8T93IViAVFjT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from transformers import pipeline\n",
        "from joblib import Parallel, delayed\n",
        "\n",
        "language_detection_pipe = pipeline(\n",
        "    \"text-classification\",\n",
        "    model=\"papluca/xlm-roberta-base-language-detection\",\n",
        "    tokenizer=\"papluca/xlm-roberta-base-language-detection\"\n",
        ")\n",
        "\n",
        "input_csv_file = \"/content/input.csv\"\n",
        "output_csv_file = \"Lang detect output.csv\"\n",
        "\n",
        "df = pd.read_csv(input_csv_file)\n",
        "\n",
        "target_ram_usage_gb = 10\n",
        "text_memory_usage_gb = 0.001\n",
        "max_batch_size = int(target_ram_usage_gb / text_memory_usage_gb)\n",
        "batch_size = min(max_batch_size, len(df))\n",
        "\n",
        "detected_languages = []\n",
        "\n",
        "average_text_length = df[\"Text\"].apply(len).mean()\n",
        "max_text_length = min(512, int(average_text_length * 1.2))\n",
        "\n",
        "def process_batch(batch_start, batch_end):\n",
        "    batch_texts = df[\"Text\"][batch_start:batch_end].tolist()\n",
        "    truncated_texts = [text[:max_text_length] if len(text) > max_text_length else text for text in batch_texts]\n",
        "    batch_languages = language_detection_pipe(truncated_texts)\n",
        "    return [result['label'] for result in batch_languages]\n",
        "\n",
        "def convert_to_full_name(initial):\n",
        "    language_names = {\n",
        "        \"af\": \"Afrikaans\",\n",
        "        \"am\": \"Amharic\",\n",
        "        \"ar\": \"Arabic\",\n",
        "        \"az\": \"Azerbaijani\",\n",
        "        \"be\": \"Belarusian\",\n",
        "        \"bg\": \"Bulgarian\",\n",
        "        \"bn\": \"Bengali\",\n",
        "        \"bs\": \"Bosnian\",\n",
        "        \"ca\": \"Catalan\",\n",
        "        \"cs\": \"Czech\",\n",
        "        \"cy\": \"Welsh\",\n",
        "        \"da\": \"Danish\",\n",
        "        \"de\": \"German\",\n",
        "        \"el\": \"Greek\",\n",
        "        \"en\": \"English\",\n",
        "        \"eo\": \"Esperanto\",\n",
        "        \"es\": \"Spanish\",\n",
        "        \"et\": \"Estonian\",\n",
        "        \"eu\": \"Basque\",\n",
        "        \"fa\": \"Persian\",\n",
        "        \"fi\": \"Finnish\",\n",
        "        \"fil\": \"Filipino\",\n",
        "        \"fr\": \"French\",\n",
        "        \"ga\": \"Irish\",\n",
        "        \"gl\": \"Galician\",\n",
        "        \"gu\": \"Gujarati\",\n",
        "        \"he\": \"Hebrew\",\n",
        "        \"hi\": \"Hindi\",\n",
        "        \"hr\": \"Croatian\",\n",
        "        \"ht\": \"Haitian Creole\",\n",
        "        \"hu\": \"Hungarian\",\n",
        "        \"hy\": \"Armenian\",\n",
        "        \"id\": \"Indonesian\",\n",
        "        \"is\": \"Icelandic\",\n",
        "        \"it\": \"Italian\",\n",
        "        \"ja\": \"Japanese\",\n",
        "        \"jv\": \"Javanese\",\n",
        "        \"ka\": \"Georgian\",\n",
        "        \"kk\": \"Kazakh\",\n",
        "        \"km\": \"Khmer\",\n",
        "        \"kn\": \"Kannada\",\n",
        "        \"ko\": \"Korean\",\n",
        "        \"ku\": \"Kurdish\",\n",
        "        \"ky\": \"Kyrgyz\",\n",
        "        \"la\": \"Latin\",\n",
        "        \"lb\": \"Luxembourgish\",\n",
        "        \"lo\": \"Lao\",\n",
        "        \"lt\": \"Lithuanian\",\n",
        "        \"lv\": \"Latvian\",\n",
        "        \"mg\": \"Malagasy\",\n",
        "        \"mi\": \"Maori\",\n",
        "        \"mk\": \"Macedonian\",\n",
        "        \"ml\": \"Malayalam\",\n",
        "        \"mn\": \"Mongolian\",\n",
        "        \"mr\": \"Marathi\",\n",
        "        \"ms\": \"Malay\",\n",
        "        \"mt\": \"Maltese\",\n",
        "        \"nb\": \"Norwegian Bokm√•l\",\n",
        "        \"ne\": \"Nepali\",\n",
        "        \"nl\": \"Dutch\",\n",
        "        \"nn\": \"Norwegian Nynorsk\",\n",
        "        \"no\": \"Norwegian\",\n",
        "        \"oc\": \"Occitan\",\n",
        "        \"or\": \"Oriya\",\n",
        "        \"pa\": \"Punjabi\",\n",
        "        \"pl\": \"Polish\",\n",
        "        \"ps\": \"Pashto\",\n",
        "        \"pt\": \"Portuguese\",\n",
        "        \"ro\": \"Romanian\",\n",
        "        \"ru\": \"Russian\",\n",
        "        \"si\": \"Sinhala\",\n",
        "        \"sk\": \"Slovak\",\n",
        "        \"sl\": \"Slovenian\",\n",
        "        \"sq\": \"Albanian\",\n",
        "        \"sr\": \"Serbian\",\n",
        "        \"sv\": \"Swedish\",\n",
        "        \"sw\": \"Swahili\",\n",
        "        \"ta\": \"Tamil\",\n",
        "        \"te\": \"Telugu\",\n",
        "        \"th\": \"Thai\",\n",
        "        \"tl\": \"Tagalog\",\n",
        "        \"tr\": \"Turkish\",\n",
        "        \"uk\": \"Ukrainian\",\n",
        "        \"ur\": \"Urdu\",\n",
        "        \"vi\": \"Vietnamese\",\n",
        "        \"xh\": \"Xhosa\",\n",
        "        \"yi\": \"Yiddish\",\n",
        "        \"yo\": \"Yoruba\",\n",
        "        \"zh\": \"Chinese\",\n",
        "        \"zu\": \"Zulu\",\n",
        "        # You can add more language mappings as needed\n",
        "    }\n",
        "    return language_names.get(initial, initial)\n",
        "\n",
        "results = Parallel(n_jobs=-1, batch_size=batch_size)(\n",
        "    delayed(process_batch)(batch_start, batch_end)\n",
        "    for batch_start in range(0, len(df), batch_size)\n",
        "    for batch_end in [min(batch_start + batch_size, len(df))]\n",
        ")\n",
        "\n",
        "for result in results:\n",
        "    detected_languages.extend(result)\n",
        "\n",
        "df[\"Detected_Language\"] = [convert_to_full_name(lang) for lang in detected_languages]\n",
        "\n",
        "df.to_csv(output_csv_file, index=False)\n",
        "\n",
        "print(\"Language detection complete. Output saved to\", output_csv_file)\n"
      ],
      "metadata": {
        "id": "Kh_DgzGEVfNC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "Ta5KWA9YW2DM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "k7ZO_ntdW2uo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}